{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","authorship_tag":"ABX9TyM+4u7jdAeQG4mJYdpk0uv0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["!pip install langchain langchain_community -q\n","!pip install google-cloud-bigquery requests -q\n","!pip install google-cloud-aiplatform vertexai scikit-learn numpy -q\n","!pip install trafilatura -q\n","!pip install customtkinter -q\n","!pip install --upgrade rich -q\n","!pip install nbstripout -q\n","\n","\n"],"metadata":{"id":"zPiRWgPqT6RU","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =============================================================================\n","# IMPORT SECTION WITH CONTEXTUAL COMMENTS\n","# =============================================================================\n","\n","# Data fetching and web content extraction utilities\n","import requests  # For HTTP requests to fetch authentication tokens and web content\n","from google.oauth2 import credentials  # Google Cloud authentication framework\n","from google.cloud import bigquery  # BigQuery client for data warehouse operations\n","from google.oauth2.credentials import Credentials  # OAuth2 credential management\n","\n","# Google Vertex AI for generative AI capabilities\n","import vertexai  # Main Vertex AI SDK initialization\n","from vertexai.generative_models import GenerativeModel  # For Gemini model integration\n","\n","# Web content extraction and text processing\n","import trafilatura  # Advanced web scraping and content extraction from URLs\n","\n","# System utilities for performance monitoring and concurrency\n","import time  # For timing operations and performance metrics\n","import threading  # For parallel execution of BigQuery queries\n","from functools import partial  # For function argument binding\n","\n","# Jupyter notebook UI components for interactive experience\n","import ipywidgets as widgets  # Interactive UI elements for question selection\n","from IPython.display import display, clear_output  # Display management in notebooks\n","\n","# Rich text formatting for enhanced terminal output\n","from rich.console import Console  # Advanced console output with formatting\n","from rich.table import Table  # Tabular data display\n","from rich.panel import Panel  # Bordered content panels\n","from rich.text import Text  # Formatted text rendering\n","from rich.markdown import Markdown  # Markdown rendering support\n","\n","\n","class NVIDIAExpertSystem:\n","    def __init__(self):\n","        \"\"\"\n","        Initialize the NVIDIA Expert System with hardcoded configuration for judges.\n","        Sets up authentication, BigQuery client, Vertex AI, and model references.\n","        This constructor handles the complete setup of the RAG pipeline components.\n","        \"\"\"\n","        # --- Hardcoded Configuration for Judges ---\n","        # Project and dataset identifiers for BigQuery operations\n","        self.EMBEDDING_MODEL = \"text_embedding_model\"\n","        self.GEMINI_MODEL = \"gemini-2.0-flash\"\n","        self.CLOUD_RUN_TOKEN_URL = \"https://bq-token-vendor-987726911762.us-central1.run.app/token\"\n","        self.GCP_PROJECT_ID = \"precise-mystery-466919-u5\"\n","        self.DATASET_ID = \"nvidia_docs_qa\"\n","        self.EMBEDDING_MODEL = \"text_embedding_model\"\n","        self.GEMINI_MODEL = \"gemini-2.0-flash\"\n","\n","        # Full table references for unified knowledge base\n","        self.NVIDIA_EMBEDDINGS_TABLE = f\"`{self.GCP_PROJECT_ID}.{self.DATASET_ID}.unified_nvidia_embeddings`\"\n","        self.NVIDIA_KNOWLEDGE_TABLE = f\"`{self.GCP_PROJECT_ID}.{self.DATASET_ID}.unified_nvidia_knowledge`\"\n","        self.SO_EMBEDDINGS_TABLE = f\"`{self.GCP_PROJECT_ID}.{self.DATASET_ID}.stackoverflow_embeddings`\"\n","        self.SO_KNOWLEDGE_TABLE = f\"`{self.GCP_PROJECT_ID}.{self.DATASET_ID}.stackoverflow_knowledge_clone`\"\n","\n","        # Model reference for embedding generation\n","        self.EMBEDDING_MODEL_REF = f\"`{self.GCP_PROJECT_ID}.{self.DATASET_ID}.{self.EMBEDDING_MODEL}`\"\n","\n","        # --- Authentication Token Acquisition ---\n","        print(f\"üîë Fetching temporary access token from: {self.CLOUD_RUN_TOKEN_URL}\")\n","        try:\n","            resp = requests.get(self.CLOUD_RUN_TOKEN_URL)\n","            resp.raise_for_status()\n","            token = resp.json()[\"access_token\"]\n","            print(\"‚úÖ Successfully fetched temporary token!\")\n","        except requests.exceptions.RequestException as e:\n","            print(f\"‚ùå ERROR: Failed to get token. Details: {e}\")\n","            raise\n","\n","        # --- BigQuery Client Initialization ---\n","        print(\"üìä Initializing BigQuery client with temporary credentials...\")\n","        creds = Credentials(token)\n","        self.client = bigquery.Client(credentials=creds, project=self.GCP_PROJECT_ID)\n","        print(\"‚úÖ BigQuery client is ready.\")\n","\n","        # --- Vertex AI Initialization ---\n","        print(\"üöÄ Initializing Vertex AI...\")\n","        vertexai_creds = Credentials(token)\n","        vertexai.init(\n","            project=self.GCP_PROJECT_ID,\n","            location=\"us-central1\",\n","            credentials=vertexai_creds\n","        )\n","        self.gen_model = GenerativeModel(self.GEMINI_MODEL)\n","        self.console = Console()\n","        print(\"‚úÖ Vertex AI initialized!\")\n","\n","    def get_embeddings_from_bigquery(self, texts):\n","        \"\"\"\n","        Convert input texts to vector embeddings using BigQuery ML's embedding model.\n","        This method handles text sanitization and executes SQL queries to generate\n","        embeddings for semantic search operations.\n","\n","        Parameters:\n","        texts (list): List of text strings to convert to embeddings\n","\n","        Returns:\n","        list: List of embedding vectors for each input text\n","        \"\"\"\n","        embeddings = []\n","        for text in texts:\n","            # Sanitize text for SQL query safety\n","            safe_text = text.replace(\"'\", \"''\").replace('\"', '\"\"')\n","\n","            # SQL query to generate embeddings using BigQuery ML\n","            query = f\"\"\"\n","            SELECT ml_generate_embedding_result\n","            FROM ML.GENERATE_EMBEDDING(\n","                MODEL {self.EMBEDDING_MODEL_REF},\n","                (SELECT '{safe_text}' AS content)\n","            )\n","            \"\"\"\n","            query_job = self.client.query(query)\n","            result = query_job.result()\n","            for row in result:\n","                embeddings.append(row.ml_generate_embedding_result)\n","        return embeddings\n","\n","    def search_similar_documents(self, question, top_k=10):\n","        \"\"\"\n","        Perform semantic search across NVIDIA documentation and Stack Overflow knowledge base.\n","        This method executes parallel vector searches against both knowledge sources\n","        and returns the most relevant documents sorted by similarity score.\n","\n","        Parameters:\n","        question (str): User query to search for\n","        top_k (int): Number of top results to return\n","\n","        Returns:\n","        list: Top matching documents sorted by similarity score\n","        \"\"\"\n","        # Generate embedding for the user question\n","        question_embedding = self.get_embeddings_from_bigquery([question])[0]\n","        embedding_str = ','.join(map(str, question_embedding))\n","\n","        # Query for NVIDIA documentation using vector search\n","        nvidia_query = f\"\"\"\n","            WITH search_results AS (\n","                SELECT\n","                    base.doc_id,\n","                    distance\n","                FROM VECTOR_SEARCH(\n","                    TABLE {self.NVIDIA_EMBEDDINGS_TABLE},\n","                    'embedding',\n","                    (SELECT [{embedding_str}] AS query_vector),\n","                    top_k => {top_k},\n","                    distance_type => 'COSINE'\n","                )\n","            )\n","            SELECT\n","                'NVIDIA Docs' AS source_type,\n","                k.content,\n","                s.distance AS similarity_score,\n","                k.source_url\n","            FROM search_results s\n","            JOIN {self.NVIDIA_KNOWLEDGE_TABLE} k\n","              ON s.doc_id = k.doc_id\n","            ORDER BY s.distance ASC\n","        \"\"\"\n","\n","        # Query for Stack Overflow questions using vector search\n","        so_query = f\"\"\"\n","            WITH search_results AS (\n","                SELECT\n","                    base.doc_id,\n","                    distance\n","                FROM VECTOR_SEARCH(\n","                    TABLE {self.SO_EMBEDDINGS_TABLE},\n","                    'embedding',\n","                    (SELECT [{embedding_str}] AS query_vector),\n","                    top_k => {top_k},\n","                    distance_type => 'COSINE'\n","                )\n","            )\n","            SELECT\n","                'Stack Overflow' AS source_type,\n","                CONCAT('**Question:** ', k.title, '\\\\n\\\\n**Answer:** ', k.answer) AS content,\n","                s.distance AS similarity_score,\n","                k.source_url\n","            FROM search_results s\n","            JOIN {self.SO_KNOWLEDGE_TABLE} k\n","              ON s.doc_id = k.doc_id\n","            ORDER BY s.distance ASC\n","        \"\"\"\n","\n","        # Execute both queries in parallel for optimal performance\n","        nvidia_job = self.client.query(nvidia_query)\n","        so_job = self.client.query(so_query)\n","\n","        # Combine and sort results by similarity score\n","        all_results = list(nvidia_job.result()) + list(so_job.result())\n","        all_results.sort(key=lambda x: x.similarity_score)\n","\n","        return all_results[:top_k]\n","\n","    def _display_search_metrics(self, docs):\n","        \"\"\"\n","        Display search results in a formatted table with confidence scores.\n","        This method visualizes the retrieval results with color-coded\n","        confidence indicators and source attribution.\n","\n","        Parameters:\n","        docs (list): List of document results from vector search\n","        \"\"\"\n","        table = Table(title=\"üìä Search & Retrieval Metrics\", show_header=True, header_style=\"bold magenta\")\n","        table.add_column(\"#\", style=\"dim\", width=3)\n","        table.add_column(\"Source\", style=\"bold blue\", width=15)\n","        table.add_column(\"Document Snippet\", style=\"cyan\", no_wrap=True, width=70)\n","        table.add_column(\"Source URL\", style=\"green\", no_wrap=True, width=50)\n","        table.add_column(\"Confidence\", justify=\"right\", style=\"bold yellow\")\n","\n","        for i, doc in enumerate(docs, 1):\n","            # Convert cosine similarity to confidence percentage\n","            confidence = (1 - doc.similarity_score) * 100\n","            snippet = doc.content.replace('\\n', ' ').strip()\n","\n","            # Color code confidence levels\n","            confidence_text = f\"{confidence:.1f}%\"\n","            if confidence > 75:\n","                color = \"green\"\n","            elif confidence > 50:\n","                color = \"yellow\"\n","            else:\n","                color = \"red\"\n","\n","            table.add_row(\n","                str(i),\n","                doc.source_type,\n","                snippet[:68] + \"...\" if len(snippet) > 70 else snippet,\n","                doc.source_url,\n","                Text(confidence_text, style=color)\n","            )\n","        self.console.print(table)\n","\n","    def _display_performance_metrics(self, timings):\n","        \"\"\"\n","        Display performance timing metrics for each stage of the RAG pipeline.\n","        This method provides transparency into the system's operational\n","        efficiency by breaking down processing times.\n","\n","        Parameters:\n","        timings (dict): Dictionary of timing measurements for each processing stage\n","        \"\"\"\n","        table = Table(title=\"‚è±Ô∏è Performance Metrics\", show_header=True, header_style=\"bold blue\")\n","        table.add_column(\"Stage\", style=\"cyan\")\n","        table.add_column(\"Duration (s)\", style=\"magenta\", justify=\"right\")\n","        table.add_column(\"Percentage\", style=\"green\", justify=\"right\")\n","\n","        total_time = sum(timings.values())\n","        for stage, duration in timings.items():\n","            percentage = (duration / total_time * 100) if total_time > 0 else 0\n","            table.add_row(stage, f\"{duration:.3f}\", f\"{percentage:.1f}%\")\n","\n","        table.add_section()\n","        table.add_row(\"Total\", f\"{total_time:.3f}\", \"100.0%\")\n","        self.console.print(table)\n","\n","    def generate_answer(self, question):\n","        \"\"\"\n","        End-to-end RAG pipeline: Search ‚Üí Context Building ‚Üí Answer Generation.\n","        This method orchestrates the complete retrieval-augmented generation\n","        process, from query processing to final answer delivery with citations.\n","\n","        Parameters:\n","        question (str): User question to process through the RAG pipeline\n","        \"\"\"\n","        timings = {}\n","\n","        # --- 1. Vector Search Phase ---\n","        start_time = time.time()\n","        similar_docs = self.search_similar_documents(question, top_k=3)\n","        timings['Vector Search'] = time.time() - start_time\n","\n","        if not similar_docs:\n","            self.console.print(\"[bold red]I couldn't find relevant information in our NVIDIA documentation.[/bold red]\")\n","            return\n","\n","        # Display search metrics to user\n","        self._display_search_metrics(similar_docs)\n","\n","        # --- 2. Context Building Phase ---\n","        start_time = time.time()\n","        context_parts = [f\"source: {doc.source_url}\\ncontent: {doc.content}\" for doc in similar_docs]\n","        sources = [f\"source: {doc.source_url}\\n\" for doc in similar_docs]\n","        timings['Content Fetching'] = time.time() - start_time\n","\n","        # Format context for LLM consumption\n","        context_text = \"\\n\\n---\\n\\n\".join(context_parts)\n","        sources_text = \"\\n\\n---\\n\\n\".join(sources)\n","        self.console.print(Panel(sources_text, title=\"[bold blue]üìù Context for LLM[/bold blue]\", expand=False))\n","\n","        # Construct RAG prompt with context and instructions\n","        prompt = f\"\"\"\n","        **Role**: You are an enthusiastic NVIDIA GPU expert assistant. You love helping developers with CUDA, GPU programming, and AI technologies.\n","\n","        **Context from NVIDIA Documentation**:\n","        {context_text}\n","\n","        **User Question**: {question}\n","\n","        **Instructions**:\n","        - Answer conversationally and helpfully, like a knowledgeable colleague.\n","        - Use bullet points or numbered steps when explaining complex topics.\n","        - Show enthusiasm for NVIDIA technologies.\n","        - Keep it professional but friendly.\n","        - Use emojis sparingly to make it engaging.\n","        - Always base your answer strictly on the context provided.\n","\n","        **Your Response**:\n","        \"\"\"\n","\n","        # --- 3. Answer Generation Phase ---\n","        start_time = time.time()\n","        response = self.gen_model.generate_content(\n","            prompt,\n","            generation_config={\n","                \"temperature\": 0.3,  # Balanced creativity vs accuracy\n","                \"max_output_tokens\": 1024,  # Optimal response length\n","                \"top_p\": 0.9  # Nucleus sampling parameter\n","            }\n","        )\n","        timings['Answer Generation'] = time.time() - start_time\n","\n","        # Display final answer with performance metrics\n","        self.console.print(Panel(Markdown(response.text), title=\"[bold green]üí° NVIDIA AI Assistant Says...[/bold green]\"))\n","        self._display_performance_metrics(timings)\n","\n","\n","def run_assistant():\n","    \"\"\"\n","    Main function to initialize and run the NVIDIA Expert System with interactive UI.\n","    This function sets up the user interface and handles the interactive question\n","    submission workflow for the RAG system.\n","    \"\"\"\n","    # Initialize the expert system\n","    expert = NVIDIAExpertSystem()\n","    expert.console.print(Panel(\"[[bold green]üöÄ NVIDIA AI Assistant Initialized[/bold green]]\", title=\"‚úÖ System Ready\", expand=False))\n","\n","    # Predefined questions for quick testing\n","    questions = [\n","        \"What is CUDA memory coalescing and why is it important?\",\n","        \"How can I optimize CUDA kernels for better performance?\",\n","        \"What are the differences between shared memory and global memory in CUDA?\",\n","        \"Explain the concept of warp divergence in CUDA.\",\n","        \"How do CUDA streams help with concurrency?\",\n","    ]\n","\n","    # --- UI Components for Interactive Experience ---\n","    question_buttons = [widgets.Button(description=q, layout=widgets.Layout(width='95%')) for q in questions]\n","    custom_question_text = widgets.Text(placeholder='Or type your own question here...', layout=widgets.Layout(width='70%'))\n","    custom_question_button = widgets.Button(description=\"Ask Assistant\", button_style='success')\n","    output_area = widgets.Output()\n","\n","    def ask_question(question_text):\n","        \"\"\"Handler function for question submission that orchestrates the answer generation process\"\"\"\n","        with output_area:\n","            clear_output()\n","            expert.console.print(Panel(f\"[bold yellow]‚ùì Asking[/bold yellow]: {question_text}\", title=\"User Question\"))\n","            expert.generate_answer(question_text)\n","\n","    def on_button_clicked(b):\n","        \"\"\"Callback for predefined question buttons that triggers the question processing pipeline\"\"\"\n","        ask_question(b.description)\n","\n","    def on_custom_button_clicked(b):\n","        \"\"\"Callback for custom question submission that validates input and initiates processing\"\"\"\n","        if custom_question_text.value:\n","            ask_question(custom_question_text.value)\n","\n","    # Register event handlers\n","    for btn in question_buttons:\n","        btn.on_click(on_button_clicked)\n","    custom_question_button.on_click(on_custom_button_clicked)\n","\n","    # --- UI Layout Assembly ---\n","    expert.console.print(Panel(\"[bold cyan]Select a question or enter your own below:[/bold cyan]\"))\n","    buttons_box = widgets.VBox(question_buttons)\n","    custom_input_box = widgets.HBox([custom_question_text, custom_question_button])\n","    display(buttons_box, custom_input_box, output_area)\n","\n","\n","if __name__ == \"__main__\":\n","    run_assistant()"],"metadata":{"id":"MAOXyDsE-8fg"},"execution_count":null,"outputs":[]}]}