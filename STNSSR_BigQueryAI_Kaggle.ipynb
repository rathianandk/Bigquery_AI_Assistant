{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","authorship_tag":"ABX9TyOtNOL9N8ZOfQ0w2LIuy7+A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b132abb34fba42a7a0b14317603398e7":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_bb381aa741b24fdcb63db6ac0f95e30c","IPY_MODEL_1ba8ca5f2f0f48c9a023b6dc907f893d","IPY_MODEL_fc1a6b53ab8a4f57b30b3978e05992a7","IPY_MODEL_42e9a79bfdaa4d228848e730d068526f","IPY_MODEL_707cc222e8a540eda3727910949c8eea"],"layout":"IPY_MODEL_1d1890a6ebcf406189c40537e7224213"}},"bb381aa741b24fdcb63db6ac0f95e30c":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"What is CUDA memory coalescing and why is it important?","disabled":false,"icon":"","layout":"IPY_MODEL_035c74a1b99740ca83fb2514a1629383","style":"IPY_MODEL_1a14136ed36240cab98c9148fa76621b","tooltip":""}},"1ba8ca5f2f0f48c9a023b6dc907f893d":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"How can I optimize CUDA kernels for better performance?","disabled":false,"icon":"","layout":"IPY_MODEL_df3374baed504e71adef6a1cebb27821","style":"IPY_MODEL_85d3500ee7ba48178634929b9983bdb6","tooltip":""}},"fc1a6b53ab8a4f57b30b3978e05992a7":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"What are the differences between shared memory and global memory in CUDA?","disabled":false,"icon":"","layout":"IPY_MODEL_007cc643ba5c4ae998d5040ed986ca9d","style":"IPY_MODEL_1d38403365a74dadbcbff08424837407","tooltip":""}},"42e9a79bfdaa4d228848e730d068526f":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Explain the concept of warp divergence in CUDA.","disabled":false,"icon":"","layout":"IPY_MODEL_4fdf967cc6f743af992527840afcc985","style":"IPY_MODEL_2215b6de9115475e9f94557d88b734ac","tooltip":""}},"707cc222e8a540eda3727910949c8eea":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"How do CUDA streams help with concurrency?","disabled":false,"icon":"","layout":"IPY_MODEL_57112c9edc824060ac2a68a0c54ed167","style":"IPY_MODEL_08f46d1d7b3546e788b10cc056e13f2f","tooltip":""}},"1d1890a6ebcf406189c40537e7224213":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"035c74a1b99740ca83fb2514a1629383":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"95%"}},"1a14136ed36240cab98c9148fa76621b":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"df3374baed504e71adef6a1cebb27821":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"95%"}},"85d3500ee7ba48178634929b9983bdb6":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"007cc643ba5c4ae998d5040ed986ca9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"95%"}},"1d38403365a74dadbcbff08424837407":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"4fdf967cc6f743af992527840afcc985":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"95%"}},"2215b6de9115475e9f94557d88b734ac":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"57112c9edc824060ac2a68a0c54ed167":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"95%"}},"08f46d1d7b3546e788b10cc056e13f2f":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"f10aa22038154e03ae27bd283e1f675a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1b95dcf36c9c44b19a0d90d33584dae7","IPY_MODEL_1bb81daf726f4009b3f8b582646a25d7"],"layout":"IPY_MODEL_e5db8aa4cfa94fb980bb37d0c7c2f90a"}},"1b95dcf36c9c44b19a0d90d33584dae7":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextView","continuous_update":true,"description":"","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_54ad074fadd94531a9340925e087721b","placeholder":"Or type your own question here...","style":"IPY_MODEL_206d7d31e56e443492ca793fd6e6cd10","value":""}},"1bb81daf726f4009b3f8b582646a25d7":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"success","description":"Ask Assistant","disabled":false,"icon":"","layout":"IPY_MODEL_eb7dd4245d924d91a41fad77bc2aa75e","style":"IPY_MODEL_dab15b5103ee4ed3b018e048ece7ec93","tooltip":""}},"e5db8aa4cfa94fb980bb37d0c7c2f90a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54ad074fadd94531a9340925e087721b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"70%"}},"206d7d31e56e443492ca793fd6e6cd10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb7dd4245d924d91a41fad77bc2aa75e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dab15b5103ee4ed3b018e048ece7ec93":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"aba8e75f39484adc9e09d388f92f2c86":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_a6e5c585276e4918a73561f575f2aac1","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"╭───────────────────────────────────────────────── User Question ─────────────────────────────────────────────────╮\n│ \u001b[1;33m❓ Asking\u001b[0m: Explain the concept of warp divergence in CUDA.                                                      │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭───────────────────────────────────────────────── User Question ─────────────────────────────────────────────────╮\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">❓ Asking</span>: Explain the concept of warp divergence in CUDA.                                                      │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[3m                                           📊 Search & Retrieval Metrics                                           \u001b[0m\n┏┳━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┓\n┃┃\u001b[1;35m \u001b[0m\u001b[1;35mSou…\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mDocument Snippet                                            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mSource URL                             \u001b[0m\u001b[1;35m \u001b[0m┃┃\n┡╇━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇┩\n││\u001b[1;34m \u001b[0m\u001b[1;34mSta…\u001b[0m\u001b[1;34m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m**Question:** When can threads of a warp get scheduled inde…\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mhttps://stackoverflow.com/questions/72…\u001b[0m\u001b[32m \u001b[0m││\n││\u001b[1;34m \u001b[0m\u001b[1;34mOve…\u001b[0m\u001b[1;34m \u001b[0m│\u001b[36m                                                              \u001b[0m│\u001b[32m                                         \u001b[0m││\n││\u001b[1;34m \u001b[0m\u001b[1;34mSta…\u001b[0m\u001b[1;34m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m**Question:** CUDA How Does Kernel Fusion Improve Performan…\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mhttps://stackoverflow.com/questions/53…\u001b[0m\u001b[32m \u001b[0m││\n││\u001b[1;34m \u001b[0m\u001b[1;34mOve…\u001b[0m\u001b[1;34m \u001b[0m│\u001b[36m                                                              \u001b[0m│\u001b[32m                                         \u001b[0m││\n││\u001b[1;34m \u001b[0m\u001b[1;34mSta…\u001b[0m\u001b[1;34m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m**Question:** Why does each thread have its own instruction…\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mhttps://stackoverflow.com/questions/58…\u001b[0m\u001b[32m \u001b[0m││\n││\u001b[1;34m \u001b[0m\u001b[1;34mOve…\u001b[0m\u001b[1;34m \u001b[0m│\u001b[36m                                                              \u001b[0m│\u001b[32m                                         \u001b[0m││\n└┴──────┴──────────────────────────────────────────────────────────────┴─────────────────────────────────────────┴┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                           📊 Search &amp; Retrieval Metrics                                           </span>\n┏┳━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┓\n┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"></span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Sou… </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Document Snippet                                             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Source URL                              </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"></span>┃\n┡╇━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇┩\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"></span>│<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Sta… </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> **Question:** When can threads of a warp get scheduled inde… </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> https://stackoverflow.com/questions/72… </span>│<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"></span>│\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"></span>│<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Ove… </span>│<span style=\"color: #008080; text-decoration-color: #008080\">                                                              </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                         </span>│<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"></span>│\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"></span>│<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Sta… </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> **Question:** CUDA How Does Kernel Fusion Improve Performan… </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> https://stackoverflow.com/questions/53… </span>│<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"></span>│\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"></span>│<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Ove… </span>│<span style=\"color: #008080; text-decoration-color: #008080\">                                                              </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                         </span>│<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"></span>│\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"></span>│<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Sta… </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> **Question:** Why does each thread have its own instruction… </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> https://stackoverflow.com/questions/58… </span>│<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"></span>│\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"></span>│<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Ove… </span>│<span style=\"color: #008080; text-decoration-color: #008080\">                                                              </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                         </span>│<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"></span>│\n└┴──────┴──────────────────────────────────────────────────────────────┴─────────────────────────────────────────┴┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"╭───────────────── \u001b[1;34m📝 Context for LLM\u001b[0m ─────────────────╮\n│ source: https://stackoverflow.com/questions/72697817 │\n│                                                      │\n│                                                      │\n│ ---                                                  │\n│                                                      │\n│ source: https://stackoverflow.com/questions/53305830 │\n│                                                      │\n│                                                      │\n│ ---                                                  │\n│                                                      │\n│ source: https://stackoverflow.com/questions/58071834 │\n│                                                      │\n╰──────────────────────────────────────────────────────╯\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭───────────────── <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">📝 Context for LLM</span> ─────────────────╮\n│ source: https://stackoverflow.com/questions/72697817 │\n│                                                      │\n│                                                      │\n│ ---                                                  │\n│                                                      │\n│ source: https://stackoverflow.com/questions/53305830 │\n│                                                      │\n│                                                      │\n│ ---                                                  │\n│                                                      │\n│ source: https://stackoverflow.com/questions/58071834 │\n│                                                      │\n╰──────────────────────────────────────────────────────╯\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"╭──────────────────────────────────────── \u001b[1;32m💡 NVIDIA AI Assistant Says...\u001b[0m ─────────────────────────────────────────╮\n│ Hey there! Warp divergence is a super interesting and important concept in CUDA programming. Let's dive into    │\n│ it! 🤿                                                                                                          │\n│                                                                                                                 │\n│ In CUDA, threads are grouped into warps, which are typically 32 threads. Ideally, all threads in a warp execute │\n│ the same instruction at the same time (in lockstep), which is the most efficient way to run code on NVIDIA      │\n│ GPUs. However, things can get a bit more complicated when threads within a warp take different execution paths  │\n│ – that's where warp divergence comes in!                                                                        │\n│                                                                                                                 │\n│ Here's the breakdown:                                                                                           │\n│                                                                                                                 │\n│ \u001b[1;33m • \u001b[0m\u001b[1mWhat is Warp Divergence?\u001b[0m Warp divergence occurs when threads within the same warp follow different control   │\n│ \u001b[1;33m   \u001b[0mflow paths, usually due to conditional statements (like \u001b[1;36;40mif\u001b[0m or \u001b[1;36;40mswitch\u001b[0m statements) where the condition         │\n│ \u001b[1;33m   \u001b[0mevaluates differently for different threads in the warp.                                                     │\n│ \u001b[1;33m • \u001b[0m\u001b[1mWhy is it a Problem?\u001b[0m When a warp diverges, the GPU has to execute each branch of the code path taken by \u001b[3many\u001b[0m  │\n│ \u001b[1;33m   \u001b[0mthread in the warp. Threads that don't satisfy the condition for a particular branch are temporarily \u001b[3mmasked \u001b[0m │\n│ \u001b[1;33m   \u001b[0m\u001b[3moff\u001b[0m (made inactive) while the other threads execute that branch. Then, the masked-off threads become active  │\n│ \u001b[1;33m   \u001b[0magain when the other branch needs to be executed. This serialization of execution paths reduces efficiency   │\n│ \u001b[1;33m   \u001b[0mbecause not all threads in the warp are actively doing useful work at the same time.                         │\n│ \u001b[1;33m • \u001b[0m\u001b[1mVolta and Later Architectures\u001b[0m: Now, here's where it gets really cool with newer architectures like Volta and │\n│ \u001b[1;33m   \u001b[0mTuring! 🎉 These architectures have a feature called \"independent thread scheduling.\" This means that each   │\n│ \u001b[1;33m   \u001b[0mthread in a warp can have its own program counter (PC) and call stack. This allows threads to diverge and    │\n│ \u001b[1;33m   \u001b[0mreconverge at a sub-warp granularity. The GPU can even yield execution of any thread to make better use of   │\n│ \u001b[1;33m   \u001b[0mexecution resources or allow one thread to wait for data produced by another.                                │\n│ \u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mHow Volta Handles Divergence\u001b[0m: Volta includes a schedule optimizer that groups active threads from the     │\n│ \u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0msame warp together into SIMT (Single Instruction, Multiple Thread) units. This maintains the high         │\n│ \u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mthroughput of SIMT execution while providing more flexibility. Threads can diverge and reconverge, and    │\n│ \u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mthe optimizer groups threads executing the same code to run them in parallel for maximum efficiency.      │\n│ \u001b[1;33m • \u001b[0m\u001b[1;36;40m__syncwarp()\u001b[0m\u001b[1m and Memory Fences\u001b[0m: When dealing with shared memory, it's important to use \u001b[1;36;40m__syncwarp()\u001b[0m to       │\n│ \u001b[1;33m   \u001b[0msynchronize threads within a warp and provide a memory fence. This prevents the compiler from optimizing     │\n│ \u001b[1;33m   \u001b[0mshared memory locations into registers and ensures that all threads in the warp have a consistent view of    │\n│ \u001b[1;33m   \u001b[0mmemory.                                                                                                      │\n│ \u001b[1;33m • \u001b[0m\u001b[1mKernel Fusion\u001b[0m: Another way to improve performance, especially in memory-bound applications, is through       │\n│ \u001b[1;33m   \u001b[0mkernel fusion. By combining multiple kernels into one, you can reduce redundant loads and stores from global │\n│ \u001b[1;33m   \u001b[0mmemory. For example, instead of loading data, adding a value, storing the result, then loading the same data │\n│ \u001b[1;33m   \u001b[0magain to multiply, you can load once, add, multiply, and then store the final result. This minimizes memory  │\n│ \u001b[1;33m   \u001b[0mtraffic and improves performance! 🚀                                                                         │\n│                                                                                                                 │\n│ In essence, warp divergence can reduce the efficiency of your CUDA code, but understanding how it works and     │\n│ leveraging features in newer architectures like Volta can help you write faster and more efficient GPU code!    │\n│ Keep experimenting and pushing the limits of what's possible with NVIDIA GPUs! 👍                               │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">💡 NVIDIA AI Assistant Says...</span> ─────────────────────────────────────────╮\n│ Hey there! Warp divergence is a super interesting and important concept in CUDA programming. Let's dive into    │\n│ it! 🤿                                                                                                          │\n│                                                                                                                 │\n│ In CUDA, threads are grouped into warps, which are typically 32 threads. Ideally, all threads in a warp execute │\n│ the same instruction at the same time (in lockstep), which is the most efficient way to run code on NVIDIA      │\n│ GPUs. However, things can get a bit more complicated when threads within a warp take different execution paths  │\n│ – that's where warp divergence comes in!                                                                        │\n│                                                                                                                 │\n│ Here's the breakdown:                                                                                           │\n│                                                                                                                 │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">What is Warp Divergence?</span> Warp divergence occurs when threads within the same warp follow different control   │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>flow paths, usually due to conditional statements (like <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">if</span> or <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">switch</span> statements) where the condition         │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>evaluates differently for different threads in the warp.                                                     │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Why is it a Problem?</span> When a warp diverges, the GPU has to execute each branch of the code path taken by <span style=\"font-style: italic\">any</span>  │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>thread in the warp. Threads that don't satisfy the condition for a particular branch are temporarily <span style=\"font-style: italic\">masked </span> │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"font-style: italic\">off</span> (made inactive) while the other threads execute that branch. Then, the masked-off threads become active  │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>again when the other branch needs to be executed. This serialization of execution paths reduces efficiency   │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>because not all threads in the warp are actively doing useful work at the same time.                         │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Volta and Later Architectures</span>: Now, here's where it gets really cool with newer architectures like Volta and │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Turing! 🎉 These architectures have a feature called \"independent thread scheduling.\" This means that each   │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>thread in a warp can have its own program counter (PC) and call stack. This allows threads to diverge and    │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>reconverge at a sub-warp granularity. The GPU can even yield execution of any thread to make better use of   │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>execution resources or allow one thread to wait for data produced by another.                                │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">How Volta Handles Divergence</span>: Volta includes a schedule optimizer that groups active threads from the     │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>same warp together into SIMT (Single Instruction, Multiple Thread) units. This maintains the high         │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>throughput of SIMT execution while providing more flexibility. Threads can diverge and reconverge, and    │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>the optimizer groups threads executing the same code to run them in parallel for maximum efficiency.      │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">__syncwarp()</span><span style=\"font-weight: bold\"> and Memory Fences</span>: When dealing with shared memory, it's important to use <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">__syncwarp()</span> to       │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>synchronize threads within a warp and provide a memory fence. This prevents the compiler from optimizing     │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>shared memory locations into registers and ensures that all threads in the warp have a consistent view of    │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>memory.                                                                                                      │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Kernel Fusion</span>: Another way to improve performance, especially in memory-bound applications, is through       │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>kernel fusion. By combining multiple kernels into one, you can reduce redundant loads and stores from global │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>memory. For example, instead of loading data, adding a value, storing the result, then loading the same data │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>again to multiply, you can load once, add, multiply, and then store the final result. This minimizes memory  │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>traffic and improves performance! 🚀                                                                         │\n│                                                                                                                 │\n│ In essence, warp divergence can reduce the efficiency of your CUDA code, but understanding how it works and     │\n│ leveraging features in newer architectures like Volta can help you write faster and more efficient GPU code!    │\n│ Keep experimenting and pushing the limits of what's possible with NVIDIA GPUs! 👍                               │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[3m              ⏱️ Performance Metrics              \u001b[0m\n┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃\u001b[1;34m \u001b[0m\u001b[1;34mStage            \u001b[0m\u001b[1;34m \u001b[0m┃\u001b[1;34m \u001b[0m\u001b[1;34mDuration (s)\u001b[0m\u001b[1;34m \u001b[0m┃\u001b[1;34m \u001b[0m\u001b[1;34mPercentage\u001b[0m\u001b[1;34m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36mVector Search    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m       2.471\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     33.7%\u001b[0m\u001b[32m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36mContent Fetching \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m       0.000\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      0.0%\u001b[0m\u001b[32m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36mAnswer Generation\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m       4.853\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     66.3%\u001b[0m\u001b[32m \u001b[0m│\n├───────────────────┼──────────────┼────────────┤\n│\u001b[36m \u001b[0m\u001b[36mTotal            \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m       7.324\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    100.0%\u001b[0m\u001b[32m \u001b[0m│\n└───────────────────┴──────────────┴────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">              ⏱️ Performance Metrics              </span>\n┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Stage             </span>┃<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Duration (s) </span>┃<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Percentage </span>┃\n┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\"> Vector Search     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">        2.471 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      33.7% </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\"> Content Fetching  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">        0.000 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       0.0% </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\"> Answer Generation </span>│<span style=\"color: #800080; text-decoration-color: #800080\">        4.853 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      66.3% </span>│\n├───────────────────┼──────────────┼────────────┤\n│<span style=\"color: #008080; text-decoration-color: #008080\"> Total             </span>│<span style=\"color: #800080; text-decoration-color: #800080\">        7.324 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     100.0% </span>│\n└───────────────────┴──────────────┴────────────┘\n</pre>\n"},"metadata":{}}]}},"a6e5c585276e4918a73561f575f2aac1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"cells":[{"cell_type":"code","source":["!pip install langchain langchain_community -q\n","!pip install google-cloud-bigquery requests\n","!pip install google-cloud-aiplatform vertexai scikit-learn numpy\n","!pip install trafilatura\n","!pip install customtkinter\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"zPiRWgPqT6RU","executionInfo":{"status":"ok","timestamp":1757873972698,"user_tz":420,"elapsed":35232,"user":{"displayName":"R A","userId":"16665474501315354161"}},"outputId":"cb799312-417d-4f6d-8199-6596d76bdfe1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.8/384.8 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.8/241.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.6/607.6 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.6/355.6 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mCollecting google-cloud-bigquery\n","  Downloading google_cloud_bigquery-3.37.0-py3-none-any.whl.metadata (8.0 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.5)\n","Requirement already satisfied: google-api-core<3.0.0,>=2.11.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (2.25.1)\n","Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.38.0)\n","Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.4.3)\n","Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.7.2)\n","Requirement already satisfied: packaging>=24.2.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (25.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.9.0.post0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.70.0)\n","Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (6.32.0)\n","Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.26.1)\n","Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.74.0)\n","Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery)\n","  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (4.9.1)\n","Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-resumable-media<3.0.0,>=2.0.0->google-cloud-bigquery) (1.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery) (1.17.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (0.6.1)\n","Downloading google_cloud_bigquery-3.37.0-py3-none-any.whl (258 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.9/258.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading grpcio_status-1.74.0-py3-none-any.whl (14 kB)\n","Installing collected packages: grpcio-status, google-cloud-bigquery\n","Successfully installed google-cloud-bigquery-3.37.0 grpcio-status-1.74.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]},"id":"e1a2cb1bf7f44aca9648cbad3984294a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting google-cloud-aiplatform\n","  Downloading google_cloud_aiplatform-1.113.0-py2.py3-none-any.whl.metadata (40 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting vertexai\n","  Downloading vertexai-1.71.1-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.25.1)\n","Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.38.0)\n","Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.26.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (6.32.0)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (25.0)\n","Requirement already satisfied: google-cloud-storage<3.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.19.0)\n","Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (3.37.0)\n","Collecting google-cloud-resource-manager<3.0.0,>=1.3.3 (from google-cloud-aiplatform)\n","  Downloading google_cloud_resource_manager-1.14.2-py3-none-any.whl.metadata (9.6 kB)\n","Collecting shapely<3.0.0 (from google-cloud-aiplatform)\n","  Downloading shapely-2.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: google-genai<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.33.0)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.11.7)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (4.15.0)\n","Requirement already satisfied: docstring_parser<1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (0.17.0)\n","Collecting google-cloud-aiplatform\n","  Downloading google_cloud_aiplatform-1.71.1-py2.py3-none-any.whl.metadata (32 kB)\n","Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 (from google-cloud-aiplatform)\n","  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n","\u001b[33mWARNING: google-cloud-aiplatform 1.71.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.70.0)\n","Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.32.5)\n","Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.74.0)\n","Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.74.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (4.9.1)\n","Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.4.3)\n","Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n","Collecting grpc-google-iam-v1<1.0.0,>=0.14.0 (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform)\n","  Downloading grpc_google_iam_v1-0.14.2-py3-none-any.whl.metadata (9.1 kB)\n","Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3.0.0,>=1.32.0->google-cloud-aiplatform) (1.7.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.4.1)\n","INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n","Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform)\n","  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n","  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n","  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n","  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n","  Downloading grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (1.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2025.8.3)\n","Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (4.10.0)\n","Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (0.28.1)\n","Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (9.1.2)\n","Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (15.0.1)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (1.3.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform) (0.16.0)\n","Downloading vertexai-1.71.1-py3-none-any.whl (7.3 kB)\n","Downloading google_cloud_aiplatform-1.71.1-py2.py3-none-any.whl (6.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading google_cloud_resource_manager-1.14.2-py3-none-any.whl (394 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.3/394.3 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shapely-2.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading grpc_google_iam_v1-0.14.2-py3-none-any.whl (19 kB)\n","Downloading grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n","Installing collected packages: shapely, protobuf, grpcio-status, grpc-google-iam-v1, google-cloud-resource-manager, google-cloud-aiplatform, vertexai\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 6.32.0\n","    Uninstalling protobuf-6.32.0:\n","      Successfully uninstalled protobuf-6.32.0\n","  Attempting uninstall: grpcio-status\n","    Found existing installation: grpcio-status 1.74.0\n","    Uninstalling grpcio-status-1.74.0:\n","      Successfully uninstalled grpcio-status-1.74.0\n","Successfully installed google-cloud-aiplatform-1.71.1 google-cloud-resource-manager-1.14.2 grpc-google-iam-v1-0.14.2 grpcio-status-1.71.2 protobuf-5.29.5 shapely-2.1.1 vertexai-1.71.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]},"id":"d78315fc6b9741029d9cd2450674da93"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting trafilatura\n","  Downloading trafilatura-2.0.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from trafilatura) (2025.8.3)\n","Requirement already satisfied: charset_normalizer>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (3.4.3)\n","Collecting courlan>=1.3.2 (from trafilatura)\n","  Downloading courlan-1.3.2-py3-none-any.whl.metadata (17 kB)\n","Collecting htmldate>=1.9.2 (from trafilatura)\n","  Downloading htmldate-1.9.3-py3-none-any.whl.metadata (10 kB)\n","Collecting justext>=3.0.1 (from trafilatura)\n","  Downloading justext-3.0.2-py2.py3-none-any.whl.metadata (7.3 kB)\n","Collecting lxml>=5.3.0 (from trafilatura)\n","  Downloading lxml-6.0.1-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n","Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (2.5.0)\n","Collecting babel>=2.16.0 (from courlan>=1.3.2->trafilatura)\n","  Downloading babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n","Collecting tld>=0.13 (from courlan>=1.3.2->trafilatura)\n","  Downloading tld-0.13.1-py2.py3-none-any.whl.metadata (10 kB)\n","Collecting dateparser>=1.1.2 (from htmldate>=1.9.2->trafilatura)\n","  Downloading dateparser-1.2.2-py3-none-any.whl.metadata (29 kB)\n","Collecting lxml>=5.3.0 (from trafilatura)\n","  Downloading lxml-5.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n","Requirement already satisfied: python-dateutil>=2.9.0.post0 in /usr/local/lib/python3.12/dist-packages (from htmldate>=1.9.2->trafilatura) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.2)\n","Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.9.1)\n","Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.3.1)\n","Collecting lxml_html_clean (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura)\n","  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura) (1.17.0)\n","Downloading trafilatura-2.0.0-py3-none-any.whl (132 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading courlan-1.3.2-py3-none-any.whl (33 kB)\n","Downloading htmldate-1.9.3-py3-none-any.whl (31 kB)\n","Downloading justext-3.0.2-py2.py3-none-any.whl (837 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.9/837.9 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lxml-5.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (5.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading babel-2.17.0-py3-none-any.whl (10.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m131.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dateparser-1.2.2-py3-none-any.whl (315 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tld-0.13.1-py2.py3-none-any.whl (274 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n","Installing collected packages: tld, lxml, babel, lxml_html_clean, dateparser, courlan, htmldate, justext, trafilatura\n","Successfully installed babel-2.17.0 courlan-1.3.2 dateparser-1.2.2 htmldate-1.9.3 justext-3.0.2 lxml-5.4.0 lxml_html_clean-0.4.2 tld-0.13.1 trafilatura-2.0.0\n","Collecting customtkinter\n","  Downloading customtkinter-5.2.2-py3-none-any.whl.metadata (677 bytes)\n","Collecting darkdetect (from customtkinter)\n","  Downloading darkdetect-0.8.0-py3-none-any.whl.metadata (3.6 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from customtkinter) (25.0)\n","Downloading customtkinter-5.2.2-py3-none-any.whl (296 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.1/296.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading darkdetect-0.8.0-py3-none-any.whl (9.0 kB)\n","Installing collected packages: darkdetect, customtkinter\n","Successfully installed customtkinter-5.2.2 darkdetect-0.8.0\n"]}]},{"cell_type":"code","source":["import requests\n","from google.oauth2 import credentials\n","from google.cloud import bigquery\n","from google.oauth2.credentials import Credentials\n","import vertexai\n","from vertexai.generative_models import GenerativeModel\n","import trafilatura # To extract content from URLs\n","import time\n","import threading\n","from functools import partial\n","import ipywidgets as widgets\n","from IPython.display import display, clear_output\n","from rich.console import Console\n","from rich.table import Table\n","from rich.panel import Panela\n","from rich.text import Text\n","from rich.markdown import Markdown\n","\n","class NVIDIAExpertSystem:\n","    def __init__(self):\n","        \"\"\"Initialize with hardcoded configuration for judges\"\"\"\n","        # --- Hardcoded Configuration for Judges ---\n","\n","        self.EMBEDDING_MODEL = \"text_embedding_model\"\n","        self.GEMINI_MODEL = \"gemini-2.0-flash\"\n","        self.CLOUD_RUN_TOKEN_URL = \"https://bq-token-vendor-987726911762.us-central1.run.app/token\"\n","        self.GCP_PROJECT_ID = \"precise-mystery-466919-u5\"\n","        self.DATASET_ID = \"nvidia_docs_qa\"\n","        self.EMBEDDING_MODEL = \"text_embedding_model\"\n","        self.GEMINI_MODEL = \"gemini-2.0-flash\"\n","         # Full table references\n","        self.NVIDIA_EMBEDDINGS_TABLE = f\"`{self.GCP_PROJECT_ID}.{self.DATASET_ID}.unified_nvidia_embeddings`\"\n","        self.NVIDIA_KNOWLEDGE_TABLE = f\"`{self.GCP_PROJECT_ID}.{self.DATASET_ID}.unified_nvidia_knowledge`\"\n","        self.SO_EMBEDDINGS_TABLE = f\"`{self.GCP_PROJECT_ID}.{self.DATASET_ID}.stackoverflow_embeddings`\"\n","        self.SO_KNOWLEDGE_TABLE = f\"`{self.GCP_PROJECT_ID}.{self.DATASET_ID}.stackoverflow_knowledge_clone`\"\n","              # self.answer_cache = {} # In-memory cache for generated answers\n","        self.EMBEDDING_MODEL_REF = f\"`{self.GCP_PROJECT_ID}.{self.DATASET_ID}.{self.EMBEDDING_MODEL}`\"\n","\n","        # --- Fetch the Short-Lived Token ---\n","        print(f\"🔑 Fetching temporary access token from: {self.CLOUD_RUN_TOKEN_URL}\")\n","        try:\n","            resp = requests.get(self.CLOUD_RUN_TOKEN_URL)\n","            resp.raise_for_status()\n","            token = resp.json()[\"access_token\"]\n","            print(\"✅ Successfully fetched temporary token!\")\n","        except requests.exceptions.RequestException as e:\n","            print(f\"❌ ERROR: Failed to get token. Details: {e}\")\n","            raise\n","\n","        # --- Initialize BigQuery Client with the Token ---\n","        print(\"📊 Initializing BigQuery client with temporary credentials...\")\n","        creds = Credentials(token)\n","        self.client = bigquery.Client(credentials=creds, project=self.GCP_PROJECT_ID)\n","        print(\"✅ BigQuery client is ready.\")\n","\n","        # --- Initialize Vertex AI ---\n","        print(\"🚀 Initializing Vertex AI...\")\n","        vertexai_creds = Credentials(token)\n","        vertexai.init(\n","            project=self.GCP_PROJECT_ID,\n","            location=\"us-central1\",\n","            credentials=vertexai_creds\n","        )\n","        self.gen_model = GenerativeModel(self.GEMINI_MODEL)\n","        self.console = Console()\n","        print(\"✅ Vertex AI initialized!\")\n","\n","    def get_embeddings_from_bigquery(self, texts):\n","        \"\"\"Get embeddings using configured embedding model\"\"\"\n","        embeddings = []\n","        for text in texts:\n","            safe_text = text.replace(\"'\", \"''\").replace('\"', '\"\"')\n","            query = f\"\"\"\n","            SELECT ml_generate_embedding_result\n","            FROM ML.GENERATE_EMBEDDING(\n","                MODEL {self.EMBEDDING_MODEL_REF},\n","                (SELECT '{safe_text}' AS content)\n","            )\n","            \"\"\"\n","            query_job = self.client.query(query)\n","            result = query_job.result()\n","            for row in result:\n","                embeddings.append(row.ml_generate_embedding_result)\n","        return embeddings\n","\n","    def search_similar_documents(self, question, top_k=10):\n","        \"\"\"Search for similar documents across NVIDIA and Stack Overflow sources.\"\"\"\n","        question_embedding = self.get_embeddings_from_bigquery([question])[0]\n","        embedding_str = ','.join(map(str, question_embedding))\n","\n","        # Query for NVIDIA documentation\n","        # Query for NVIDIA docs\n","        nvidia_query = f\"\"\"\n","            WITH search_results AS (\n","                SELECT\n","                    base.doc_id,\n","                    distance\n","                FROM VECTOR_SEARCH(\n","                    TABLE {self.NVIDIA_EMBEDDINGS_TABLE},\n","                    'embedding',\n","                    (SELECT [{embedding_str}] AS query_vector),\n","                    top_k => {top_k},\n","                    distance_type => 'COSINE'\n","                )\n","            )\n","            SELECT\n","                'NVIDIA Docs' AS source_type,\n","                k.content,\n","                s.distance AS similarity_score,\n","                k.source_url\n","            FROM search_results s\n","            JOIN {self.NVIDIA_KNOWLEDGE_TABLE} k\n","              ON s.doc_id = k.doc_id\n","            ORDER BY s.distance ASC\n","        \"\"\"\n","\n","        # Query for Stack Overflow questions\n","        so_query = f\"\"\"\n","            WITH search_results AS (\n","                SELECT\n","                    base.doc_id,\n","                    distance\n","                FROM VECTOR_SEARCH(\n","                    TABLE {self.SO_EMBEDDINGS_TABLE},\n","                    'embedding',\n","                    (SELECT [{embedding_str}] AS query_vector),\n","                    top_k => {top_k},\n","                    distance_type => 'COSINE'\n","                )\n","            )\n","            SELECT\n","                'Stack Overflow' AS source_type,\n","                CONCAT('**Question:** ', k.title, '\\\\n\\\\n**Answer:** ', k.answer) AS content,\n","                s.distance AS similarity_score,\n","                k.source_url\n","            FROM search_results s\n","            JOIN {self.SO_KNOWLEDGE_TABLE} k\n","              ON s.doc_id = k.doc_id\n","            ORDER BY s.distance ASC\n","        \"\"\"\n","\n","\n","        # Run queries in parallel\n","        nvidia_job = self.client.query(nvidia_query)\n","        so_job = self.client.query(so_query)\n","\n","        # Combine and sort results\n","        all_results = list(nvidia_job.result()) + list(so_job.result())\n","        all_results.sort(key=lambda x: x.similarity_score)\n","\n","        return all_results[:top_k]\n","\n","    def _display_search_metrics(self, docs):\n","        table = Table(title=\"📊 Search & Retrieval Metrics\", show_header=True, header_style=\"bold magenta\")\n","        table.add_column(\"#\", style=\"dim\", width=3)\n","        table.add_column(\"Source\", style=\"bold blue\", width=15)\n","        table.add_column(\"Document Snippet\", style=\"cyan\", no_wrap=True, width=70)\n","        table.add_column(\"Source URL\", style=\"green\", no_wrap=True, width=50)\n","        table.add_column(\"Confidence\", justify=\"right\", style=\"bold yellow\")\n","\n","        for i, doc in enumerate(docs, 1):\n","            confidence = (1 - doc.similarity_score) * 100\n","            snippet = doc.content.replace('\\n', ' ').strip()\n","\n","            confidence_text = f\"{confidence:.1f}%\"\n","            if confidence > 75:\n","                color = \"green\"\n","            elif confidence > 50:\n","                color = \"yellow\"\n","            else:\n","                color = \"red\"\n","\n","            table.add_row(\n","                str(i),\n","                doc.source_type,\n","                snippet[:68] + \"...\" if len(snippet) > 70 else snippet,\n","                doc.source_url,\n","                Text(confidence_text, style=color)\n","            )\n","        self.console.print(table)\n","\n","    def _display_performance_metrics(self, timings):\n","        table = Table(title=\"⏱️ Performance Metrics\", show_header=True, header_style=\"bold blue\")\n","        table.add_column(\"Stage\", style=\"cyan\")\n","        table.add_column(\"Duration (s)\", style=\"magenta\", justify=\"right\")\n","        table.add_column(\"Percentage\", style=\"green\", justify=\"right\")\n","\n","        total_time = sum(timings.values())\n","        for stage, duration in timings.items():\n","            percentage = (duration / total_time * 100) if total_time > 0 else 0\n","            table.add_row(stage, f\"{duration:.3f}\", f\"{percentage:.1f}%\")\n","\n","        table.add_section()\n","        table.add_row(\"Total\", f\"{total_time:.3f}\", \"100.0%\")\n","        self.console.print(table)\n","\n","    def generate_answer(self, question):\n","        \"\"\"Generate conversational answer using ONLY BigQuery embeddings context\"\"\"\n","\n","        timings = {}\n","\n","        # --- 1. Vector Search ---\n","        start_time = time.time()\n","        similar_docs = self.search_similar_documents(question, top_k=3)\n","        timings['Vector Search'] = time.time() - start_time\n","\n","        if not similar_docs:\n","            self.console.print(\"[bold red]I couldn't find relevant information in our NVIDIA documentation.[/bold red]\")\n","            return\n","\n","        self._display_search_metrics(similar_docs)\n","\n","        # --- 2. Build context from search results ---\n","        start_time = time.time()\n","        context_parts = [f\"source: {doc.source_url}\\ncontent: {doc.content}\" for doc in similar_docs]\n","        sources = [f\"source: {doc.source_url}\\n\" for doc in similar_docs]\n","        timings['Content Fetching'] = time.time() - start_time\n","        context_text = \"\\n\\n---\\n\\n\".join(context_parts)\n","        sources_text = \"\\n\\n---\\n\\n\".join(sources)\n","        self.console.print(Panel(sources_text, title=\"[bold blue]📝 Context for LLM[/bold blue]\", expand=False))\n","\n","        prompt = f\"\"\"\n","        **Role**: You are an enthusiastic NVIDIA GPU expert assistant. You love helping developers with CUDA, GPU programming, and AI technologies.\n","\n","        **Context from NVIDIA Documentation**:\n","        {context_text}\n","\n","        **User Question**: {question}\n","\n","        **Instructions**:\n","        - Answer conversationally and helpfully, like a knowledgeable colleague.\n","        - Use bullet points or numbered steps when explaining complex topics.\n","        - Show enthusiasm for NVIDIA technologies.\n","        - Keep it professional but friendly.\n","        - Use emojis sparingly to make it engaging.\n","        - Always base your answer strictly on the context provided.\n","\n","        **Your Response**:\n","        \"\"\"\n","\n","        # --- 3. Answer Generation ---\n","        start_time = time.time()\n","        response = self.gen_model.generate_content(\n","            prompt,\n","            generation_config={\n","                \"temperature\": 0.3,\n","                \"max_output_tokens\": 1024,\n","                \"top_p\": 0.9\n","            }\n","        )\n","        timings['Answer Generation'] = time.time() - start_time\n","\n","        self.console.print(Panel(Markdown(response.text), title=\"[bold green]💡 NVIDIA AI Assistant Says...[/bold green]\"))\n","        self._display_performance_metrics(timings)\n","\n","def run_assistant():\n","    expert = NVIDIAExpertSystem()\n","    expert.console.print(Panel(\"[[bold green]🚀 NVIDIA AI Assistant Initialized[/bold green]]\", title=\"✅ System Ready\", expand=False))\n","\n","    questions = [\n","        \"What is CUDA memory coalescing and why is it important?\",\n","        \"How can I optimize CUDA kernels for better performance?\",\n","        \"What are the differences between shared memory and global memory in CUDA?\",\n","        \"Explain the concept of warp divergence in CUDA.\",\n","        \"How do CUDA streams help with concurrency?\",\n","    ]\n","\n","    # --- UI Components ---\n","    question_buttons = [widgets.Button(description=q, layout=widgets.Layout(width='95%')) for q in questions]\n","    custom_question_text = widgets.Text(placeholder='Or type your own question here...', layout=widgets.Layout(width='70%'))\n","    custom_question_button = widgets.Button(description=\"Ask Assistant\", button_style='success')\n","    output_area = widgets.Output()\n","\n","    def ask_question(question_text):\n","        with output_area:\n","            clear_output()\n","            expert.console.print(Panel(f\"[bold yellow]❓ Asking[/bold yellow]: {question_text}\", title=\"User Question\"))\n","            expert.generate_answer(question_text)\n","\n","    def on_button_clicked(b):\n","        ask_question(b.description)\n","\n","    def on_custom_button_clicked(b):\n","        if custom_question_text.value:\n","            ask_question(custom_question_text.value)\n","\n","    for btn in question_buttons:\n","        btn.on_click(on_button_clicked)\n","    custom_question_button.on_click(on_custom_button_clicked)\n","\n","    # --- Layout ---\n","    expert.console.print(Panel(\"[bold cyan]Select a question or enter your own below:[/bold cyan]\"))\n","    buttons_box = widgets.VBox(question_buttons)\n","    custom_input_box = widgets.HBox([custom_question_text, custom_question_button])\n","    display(buttons_box, custom_input_box, output_area)\n","\n","if __name__ == \"__main__\":\n","    run_assistant()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["b132abb34fba42a7a0b14317603398e7","bb381aa741b24fdcb63db6ac0f95e30c","1ba8ca5f2f0f48c9a023b6dc907f893d","fc1a6b53ab8a4f57b30b3978e05992a7","42e9a79bfdaa4d228848e730d068526f","707cc222e8a540eda3727910949c8eea","1d1890a6ebcf406189c40537e7224213","035c74a1b99740ca83fb2514a1629383","1a14136ed36240cab98c9148fa76621b","df3374baed504e71adef6a1cebb27821","85d3500ee7ba48178634929b9983bdb6","007cc643ba5c4ae998d5040ed986ca9d","1d38403365a74dadbcbff08424837407","4fdf967cc6f743af992527840afcc985","2215b6de9115475e9f94557d88b734ac","57112c9edc824060ac2a68a0c54ed167","08f46d1d7b3546e788b10cc056e13f2f","f10aa22038154e03ae27bd283e1f675a","1b95dcf36c9c44b19a0d90d33584dae7","1bb81daf726f4009b3f8b582646a25d7","e5db8aa4cfa94fb980bb37d0c7c2f90a","54ad074fadd94531a9340925e087721b","206d7d31e56e443492ca793fd6e6cd10","eb7dd4245d924d91a41fad77bc2aa75e","dab15b5103ee4ed3b018e048ece7ec93","aba8e75f39484adc9e09d388f92f2c86","a6e5c585276e4918a73561f575f2aac1"]},"id":"MAOXyDsE-8fg","executionInfo":{"status":"ok","timestamp":1757810158989,"user_tz":420,"elapsed":255,"user":{"displayName":"R A","userId":"16665474501315354161"}},"outputId":"f424cb72-3f3b-4ed1-e498-403be90c6fa4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["🔑 Fetching temporary access token from: https://bq-token-vendor-987726911762.us-central1.run.app/token\n","✅ Successfully fetched temporary token!\n","📊 Initializing BigQuery client with temporary credentials...\n","✅ BigQuery client is ready.\n","🚀 Initializing Vertex AI...\n","✅ Vertex AI initialized!\n"]},{"output_type":"display_data","data":{"text/plain":["╭────────── ✅ System Ready ───────────╮\n","│ [\u001b[1;32m🚀 NVIDIA AI Assistant Initialized\u001b[0m] │\n","╰──────────────────────────────────────╯\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭────────── ✅ System Ready ───────────╮\n","│ [<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">🚀 NVIDIA AI Assistant Initialized</span>] │\n","╰──────────────────────────────────────╯\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n","│ \u001b[1;36mSelect a question or enter your own below:\u001b[0m                                                                      │\n","╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n","│ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Select a question or enter your own below:</span>                                                                      │\n","╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Button(description='What is CUDA memory coalescing and why is it important?', layout=Layout(wid…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b132abb34fba42a7a0b14317603398e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["HBox(children=(Text(value='', layout=Layout(width='70%'), placeholder='Or type your own question here...'), Bu…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f10aa22038154e03ae27bd283e1f675a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aba8e75f39484adc9e09d388f92f2c86"}},"metadata":{}}]}]}