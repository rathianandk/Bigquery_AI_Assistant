{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","authorship_tag":"ABX9TyMb34VvlsgpZpFLHqRZ48ty"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f06b74b2cb694f379ee3832800e53d34":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_efbe3955664a4e0f9796499e06cc41c6","IPY_MODEL_347964ad3f844cb38126bf474be4a4f0","IPY_MODEL_76170ad5d44948599f1cd9ccdcbf95be","IPY_MODEL_7403dc4df4bf41d7b69f743d06f6dfc3","IPY_MODEL_3214db75609a46ffb381c133e45a1e29"],"layout":"IPY_MODEL_daf000d2a5f241b2b6fb31569d5a2791"}},"efbe3955664a4e0f9796499e06cc41c6":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"What is CUDA memory coalescing and why is it important?","disabled":false,"icon":"","layout":"IPY_MODEL_e3d42cb97e8849eb9e9547328534a266","style":"IPY_MODEL_b4b84e33b2a94d188b22a401e117010c","tooltip":""}},"347964ad3f844cb38126bf474be4a4f0":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"How can I optimize CUDA kernels for better performance?","disabled":false,"icon":"","layout":"IPY_MODEL_21d9ca7b628643c29c240985ae1af879","style":"IPY_MODEL_0bf1813ad31b4b5d9ffb627eb1469d06","tooltip":""}},"76170ad5d44948599f1cd9ccdcbf95be":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"What are the differences between shared memory and global memory in CUDA?","disabled":false,"icon":"","layout":"IPY_MODEL_23ec73589ef74bbb9a0208b6ec6ea31a","style":"IPY_MODEL_3bced61e52e8457c8a393ee2d9e7b40a","tooltip":""}},"7403dc4df4bf41d7b69f743d06f6dfc3":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Explain the concept of warp divergence in CUDA.","disabled":false,"icon":"","layout":"IPY_MODEL_7b60cf4b580e4e14afd20da36b767ed5","style":"IPY_MODEL_804269e39f8a45b7bbdf6e1649065ac1","tooltip":""}},"3214db75609a46ffb381c133e45a1e29":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"How do CUDA streams help with concurrency?","disabled":false,"icon":"","layout":"IPY_MODEL_545f47e4541148bf85e4c3e0f4a75c52","style":"IPY_MODEL_7118b07b7e974b688d0be610af7049f7","tooltip":""}},"daf000d2a5f241b2b6fb31569d5a2791":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3d42cb97e8849eb9e9547328534a266":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"95%"}},"b4b84e33b2a94d188b22a401e117010c":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"21d9ca7b628643c29c240985ae1af879":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"95%"}},"0bf1813ad31b4b5d9ffb627eb1469d06":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"23ec73589ef74bbb9a0208b6ec6ea31a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"95%"}},"3bced61e52e8457c8a393ee2d9e7b40a":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"7b60cf4b580e4e14afd20da36b767ed5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"95%"}},"804269e39f8a45b7bbdf6e1649065ac1":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"545f47e4541148bf85e4c3e0f4a75c52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"95%"}},"7118b07b7e974b688d0be610af7049f7":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"c6f64945e6f94a63bb9033f59f35ba7d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4df05d0b5c71458aade0df44f2064ecd","IPY_MODEL_068dae937a3a41eea667ca46f76aa099"],"layout":"IPY_MODEL_8427c50db6a141d88947b85ed155c22c"}},"4df05d0b5c71458aade0df44f2064ecd":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextView","continuous_update":true,"description":"","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_68ab082458a549358e25080469864c16","placeholder":"Or type your own question here...","style":"IPY_MODEL_65344159193f4d6da56192d676937277","value":""}},"068dae937a3a41eea667ca46f76aa099":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"success","description":"Ask Assistant","disabled":false,"icon":"","layout":"IPY_MODEL_aa8e5e85b7da454cbbeba07f0c20ac5d","style":"IPY_MODEL_90d5bc10f06745859a9c6837d7344950","tooltip":""}},"8427c50db6a141d88947b85ed155c22c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68ab082458a549358e25080469864c16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"70%"}},"65344159193f4d6da56192d676937277":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa8e5e85b7da454cbbeba07f0c20ac5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90d5bc10f06745859a9c6837d7344950":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"10783e6972514b4eab5e373d8fc10bc8":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_44d27420a8794269af92f6f9a23203f0","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"╭───────────────────────────────────────────────── User Question ─────────────────────────────────────────────────╮\n│ \u001b[1;33m❓ Asking\u001b[0m: What is CUDA memory coalescing and why is it important?                                              │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭───────────────────────────────────────────────── User Question ─────────────────────────────────────────────────╮\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">❓ Asking</span>: What is CUDA memory coalescing and why is it important?                                              │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[3m                                           📊 Search & Retrieval Metrics                                           \u001b[0m\n┏┳━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┓\n┃┃\u001b[1;35m \u001b[0m\u001b[1;35mSou…\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mDocument Snippet                                            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mSource URL                             \u001b[0m\u001b[1;35m \u001b[0m┃┃\n┡╇━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇┩\n││\u001b[1;34m \u001b[0m\u001b[1;34mSta…\u001b[0m\u001b[1;34m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m**Question:** Memory Coalescing vs. Vectorized Memory Acces…\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mhttps://stackoverflow.com/questions/56…\u001b[0m\u001b[32m \u001b[0m││\n││\u001b[1;34m \u001b[0m\u001b[1;34mOve…\u001b[0m\u001b[1;34m \u001b[0m│\u001b[36m                                                              \u001b[0m│\u001b[32m                                         \u001b[0m││\n││\u001b[1;34m \u001b[0m\u001b[1;34mSta…\u001b[0m\u001b[1;34m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m**Question:** How do GPU cores communicate with each other?…\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mhttps://stackoverflow.com/questions/64…\u001b[0m\u001b[32m \u001b[0m││\n││\u001b[1;34m \u001b[0m\u001b[1;34mOve…\u001b[0m\u001b[1;34m \u001b[0m│\u001b[36m                                                              \u001b[0m│\u001b[32m                                         \u001b[0m││\n││\u001b[1;34m \u001b[0m\u001b[1;34mSta…\u001b[0m\u001b[1;34m \u001b[0m│\u001b[36m \u001b[0m\u001b[36m**Question:** CUDA How Does Kernel Fusion Improve Performan…\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mhttps://stackoverflow.com/questions/53…\u001b[0m\u001b[32m \u001b[0m││\n││\u001b[1;34m \u001b[0m\u001b[1;34mOve…\u001b[0m\u001b[1;34m \u001b[0m│\u001b[36m                                                              \u001b[0m│\u001b[32m                                         \u001b[0m││\n└┴──────┴──────────────────────────────────────────────────────────────┴─────────────────────────────────────────┴┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                           📊 Search &amp; Retrieval Metrics                                           </span>\n┏┳━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳┓\n┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"></span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Sou… </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Document Snippet                                             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Source URL                              </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"></span>┃\n┡╇━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇┩\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"></span>│<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Sta… </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> **Question:** Memory Coalescing vs. Vectorized Memory Acces… </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> https://stackoverflow.com/questions/56… </span>│<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"></span>│\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"></span>│<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Ove… </span>│<span style=\"color: #008080; text-decoration-color: #008080\">                                                              </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                         </span>│<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"></span>│\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"></span>│<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Sta… </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> **Question:** How do GPU cores communicate with each other?… </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> https://stackoverflow.com/questions/64… </span>│<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"></span>│\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"></span>│<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Ove… </span>│<span style=\"color: #008080; text-decoration-color: #008080\">                                                              </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                         </span>│<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"></span>│\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"></span>│<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Sta… </span>│<span style=\"color: #008080; text-decoration-color: #008080\"> **Question:** CUDA How Does Kernel Fusion Improve Performan… </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> https://stackoverflow.com/questions/53… </span>│<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"></span>│\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"></span>│<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Ove… </span>│<span style=\"color: #008080; text-decoration-color: #008080\">                                                              </span>│<span style=\"color: #008000; text-decoration-color: #008000\">                                         </span>│<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"></span>│\n└┴──────┴──────────────────────────────────────────────────────────────┴─────────────────────────────────────────┴┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"╭───────────────── \u001b[1;34m📝 Context for LLM\u001b[0m ─────────────────╮\n│ source: https://stackoverflow.com/questions/56966466 │\n│                                                      │\n│                                                      │\n│ ---                                                  │\n│                                                      │\n│ source: https://stackoverflow.com/questions/64846670 │\n│                                                      │\n│                                                      │\n│ ---                                                  │\n│                                                      │\n│ source: https://stackoverflow.com/questions/53305830 │\n│                                                      │\n╰──────────────────────────────────────────────────────╯\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭───────────────── <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">📝 Context for LLM</span> ─────────────────╮\n│ source: https://stackoverflow.com/questions/56966466 │\n│                                                      │\n│                                                      │\n│ ---                                                  │\n│                                                      │\n│ source: https://stackoverflow.com/questions/64846670 │\n│                                                      │\n│                                                      │\n│ ---                                                  │\n│                                                      │\n│ source: https://stackoverflow.com/questions/53305830 │\n│                                                      │\n╰──────────────────────────────────────────────────────╯\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"╭──────────────────────────────────────── \u001b[1;32m💡 NVIDIA AI Assistant Says...\u001b[0m ─────────────────────────────────────────╮\n│ Hey there! 👋 Let's dive into CUDA memory coalescing! It's a super important optimization technique when you're │\n│ working with NVIDIA GPUs.                                                                                       │\n│                                                                                                                 │\n│ Think of memory coalescing as a way to make sure your GPU threads work together efficiently when accessing      │\n│ global memory. 🤝                                                                                               │\n│                                                                                                                 │\n│ Here's the breakdown:                                                                                           │\n│                                                                                                                 │\n│ \u001b[1;33m • \u001b[0m\u001b[1mWhat it is:\u001b[0m Memory coalescing is essentially about grouping memory accesses from multiple threads into a     │\n│ \u001b[1;33m   \u001b[0msingle, larger transaction. This is similar to what short-vector CPU SIMD does at compile time, but the GPU  │\n│ \u001b[1;33m   \u001b[0mdoes it at runtime. The goal is to maximize memory throughput by reducing the number of separate memory      │\n│ \u001b[1;33m   \u001b[0mtransactions.                                                                                                │\n│ \u001b[1;33m • \u001b[0m\u001b[1mWhy it's important:\u001b[0m GPUs are designed for massively parallel processing, where many threads execute the same │\n│ \u001b[1;33m   \u001b[0mcode on different data. If these threads access memory in a scattered or unorganized way, it can lead to a   │\n│ \u001b[1;33m   \u001b[0mlot of wasted time and bandwidth. Memory coalescing ensures that when threads in a warp (a group of 32       │\n│ \u001b[1;33m   \u001b[0mthreads in NVIDIA GPUs) access memory, those accesses are aligned and contiguous, resulting in fewer,        │\n│ \u001b[1;33m   \u001b[0mlarger, and more efficient memory transactions. 🚀                                                           │\n│ \u001b[1;33m • \u001b[0m\u001b[1mHow it helps:\u001b[0m By coalescing memory accesses, you're essentially optimizing how data is fetched from global   │\n│ \u001b[1;33m   \u001b[0mmemory. This is especially crucial for memory-bound applications, where the speed of memory access is the    │\n│ \u001b[1;33m   \u001b[0mbottleneck.                                                                                                  │\n│                                                                                                                 │\n│ To put it simply, memory coalescing is like packing items efficiently into a box 📦. Instead of sending many    │\n│ small packages, you combine them into a few larger ones, saving time and resources. This optimization is key to │\n│ unlocking the full potential of NVIDIA GPUs! 🤩                                                                 │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────── <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">💡 NVIDIA AI Assistant Says...</span> ─────────────────────────────────────────╮\n│ Hey there! 👋 Let's dive into CUDA memory coalescing! It's a super important optimization technique when you're │\n│ working with NVIDIA GPUs.                                                                                       │\n│                                                                                                                 │\n│ Think of memory coalescing as a way to make sure your GPU threads work together efficiently when accessing      │\n│ global memory. 🤝                                                                                               │\n│                                                                                                                 │\n│ Here's the breakdown:                                                                                           │\n│                                                                                                                 │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">What it is:</span> Memory coalescing is essentially about grouping memory accesses from multiple threads into a     │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>single, larger transaction. This is similar to what short-vector CPU SIMD does at compile time, but the GPU  │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>does it at runtime. The goal is to maximize memory throughput by reducing the number of separate memory      │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>transactions.                                                                                                │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Why it's important:</span> GPUs are designed for massively parallel processing, where many threads execute the same │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>code on different data. If these threads access memory in a scattered or unorganized way, it can lead to a   │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>lot of wasted time and bandwidth. Memory coalescing ensures that when threads in a warp (a group of 32       │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>threads in NVIDIA GPUs) access memory, those accesses are aligned and contiguous, resulting in fewer,        │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>larger, and more efficient memory transactions. 🚀                                                           │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">How it helps:</span> By coalescing memory accesses, you're essentially optimizing how data is fetched from global   │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>memory. This is especially crucial for memory-bound applications, where the speed of memory access is the    │\n│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>bottleneck.                                                                                                  │\n│                                                                                                                 │\n│ To put it simply, memory coalescing is like packing items efficiently into a box 📦. Instead of sending many    │\n│ small packages, you combine them into a few larger ones, saving time and resources. This optimization is key to │\n│ unlocking the full potential of NVIDIA GPUs! 🤩                                                                 │\n╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[3m              ⏱️ Performance Metrics              \u001b[0m\n┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃\u001b[1;34m \u001b[0m\u001b[1;34mStage            \u001b[0m\u001b[1;34m \u001b[0m┃\u001b[1;34m \u001b[0m\u001b[1;34mDuration (s)\u001b[0m\u001b[1;34m \u001b[0m┃\u001b[1;34m \u001b[0m\u001b[1;34mPercentage\u001b[0m\u001b[1;34m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36mVector Search    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m       9.919\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     77.3%\u001b[0m\u001b[32m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36mContent Fetching \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m       0.000\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      0.0%\u001b[0m\u001b[32m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36mAnswer Generation\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m       2.915\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     22.7%\u001b[0m\u001b[32m \u001b[0m│\n├───────────────────┼──────────────┼────────────┤\n│\u001b[36m \u001b[0m\u001b[36mTotal            \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      12.834\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    100.0%\u001b[0m\u001b[32m \u001b[0m│\n└───────────────────┴──────────────┴────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">              ⏱️ Performance Metrics              </span>\n┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Stage             </span>┃<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Duration (s) </span>┃<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Percentage </span>┃\n┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\"> Vector Search     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">        9.919 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      77.3% </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\"> Content Fetching  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">        0.000 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       0.0% </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\"> Answer Generation </span>│<span style=\"color: #800080; text-decoration-color: #800080\">        2.915 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      22.7% </span>│\n├───────────────────┼──────────────┼────────────┤\n│<span style=\"color: #008080; text-decoration-color: #008080\"> Total             </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       12.834 </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     100.0% </span>│\n└───────────────────┴──────────────┴────────────┘\n</pre>\n"},"metadata":{}}]}},"44d27420a8794269af92f6f9a23203f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"cells":[{"cell_type":"code","source":["!pip install langchain langchain_community -q\n","!pip install google-cloud-bigquery requests -q\n","!pip install google-cloud-aiplatform vertexai scikit-learn numpy -q\n","!pip install trafilatura -q\n","!pip install customtkinter -q\n","!pip install --upgrade rich -q\n","!pip install nbstripout -q\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPiRWgPqT6RU","executionInfo":{"status":"ok","timestamp":1758058308638,"user_tz":420,"elapsed":14660,"user":{"displayName":"R A","userId":"16665474501315354161"}},"outputId":"0edf36aa-8af3-4262-f32a-fed41f998b2e","collapsed":true},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: google-cloud-aiplatform 1.71.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import requests\n","from google.oauth2 import credentials\n","from google.cloud import bigquery\n","from google.oauth2.credentials import Credentials\n","import vertexai\n","from vertexai.generative_models import GenerativeModel\n","import trafilatura # To extract content from URLs\n","import time\n","import threading\n","from functools import partial\n","import ipywidgets as widgets\n","from IPython.display import display, clear_output\n","from rich.console import Console\n","from rich.table import Table\n","from rich.panel import Panel\n","from rich.text import Text\n","from rich.markdown import Markdown\n","\n","class NVIDIAExpertSystem:\n","    def __init__(self):\n","        \"\"\"Initialize with hardcoded configuration for judges\"\"\"\n","        # --- Hardcoded Configuration for Judges ---\n","\n","        self.EMBEDDING_MODEL = \"text_embedding_model\"\n","        self.GEMINI_MODEL = \"gemini-2.0-flash\"\n","        self.CLOUD_RUN_TOKEN_URL = \"https://bq-token-vendor-987726911762.us-central1.run.app/token\"\n","        self.GCP_PROJECT_ID = \"precise-mystery-466919-u5\"\n","        self.DATASET_ID = \"nvidia_docs_qa\"\n","        self.EMBEDDING_MODEL = \"text_embedding_model\"\n","        self.GEMINI_MODEL = \"gemini-2.0-flash\"\n","         # Full table references\n","        self.NVIDIA_EMBEDDINGS_TABLE = f\"`{self.GCP_PROJECT_ID}.{self.DATASET_ID}.unified_nvidia_embeddings`\"\n","        self.NVIDIA_KNOWLEDGE_TABLE = f\"`{self.GCP_PROJECT_ID}.{self.DATASET_ID}.unified_nvidia_knowledge`\"\n","        self.SO_EMBEDDINGS_TABLE = f\"`{self.GCP_PROJECT_ID}.{self.DATASET_ID}.stackoverflow_embeddings`\"\n","        self.SO_KNOWLEDGE_TABLE = f\"`{self.GCP_PROJECT_ID}.{self.DATASET_ID}.stackoverflow_knowledge_clone`\"\n","              # self.answer_cache = {} # In-memory cache for generated answers\n","        self.EMBEDDING_MODEL_REF = f\"`{self.GCP_PROJECT_ID}.{self.DATASET_ID}.{self.EMBEDDING_MODEL}`\"\n","\n","        # --- Fetch the Short-Lived Token ---\n","        print(f\"🔑 Fetching temporary access token from: {self.CLOUD_RUN_TOKEN_URL}\")\n","        try:\n","            resp = requests.get(self.CLOUD_RUN_TOKEN_URL)\n","            resp.raise_for_status()\n","            token = resp.json()[\"access_token\"]\n","            print(\"✅ Successfully fetched temporary token!\")\n","        except requests.exceptions.RequestException as e:\n","            print(f\"❌ ERROR: Failed to get token. Details: {e}\")\n","            raise\n","\n","        # --- Initialize BigQuery Client with the Token ---\n","        print(\"📊 Initializing BigQuery client with temporary credentials...\")\n","        creds = Credentials(token)\n","        self.client = bigquery.Client(credentials=creds, project=self.GCP_PROJECT_ID)\n","        print(\"✅ BigQuery client is ready.\")\n","\n","        # --- Initialize Vertex AI ---\n","        print(\"🚀 Initializing Vertex AI...\")\n","        vertexai_creds = Credentials(token)\n","        vertexai.init(\n","            project=self.GCP_PROJECT_ID,\n","            location=\"us-central1\",\n","            credentials=vertexai_creds\n","        )\n","        self.gen_model = GenerativeModel(self.GEMINI_MODEL)\n","        self.console = Console()\n","        print(\"✅ Vertex AI initialized!\")\n","\n","    def get_embeddings_from_bigquery(self, texts):\n","        \"\"\"Get embeddings using configured embedding model\"\"\"\n","        embeddings = []\n","        for text in texts:\n","            safe_text = text.replace(\"'\", \"''\").replace('\"', '\"\"')\n","            query = f\"\"\"\n","            SELECT ml_generate_embedding_result\n","            FROM ML.GENERATE_EMBEDDING(\n","                MODEL {self.EMBEDDING_MODEL_REF},\n","                (SELECT '{safe_text}' AS content)\n","            )\n","            \"\"\"\n","            query_job = self.client.query(query)\n","            result = query_job.result()\n","            for row in result:\n","                embeddings.append(row.ml_generate_embedding_result)\n","        return embeddings\n","\n","    def search_similar_documents(self, question, top_k=10):\n","        \"\"\"Search for similar documents across NVIDIA and Stack Overflow sources.\"\"\"\n","        question_embedding = self.get_embeddings_from_bigquery([question])[0]\n","        embedding_str = ','.join(map(str, question_embedding))\n","\n","        # Query for NVIDIA documentation\n","        # Query for NVIDIA docs\n","        nvidia_query = f\"\"\"\n","            WITH search_results AS (\n","                SELECT\n","                    base.doc_id,\n","                    distance\n","                FROM VECTOR_SEARCH(\n","                    TABLE {self.NVIDIA_EMBEDDINGS_TABLE},\n","                    'embedding',\n","                    (SELECT [{embedding_str}] AS query_vector),\n","                    top_k => {top_k},\n","                    distance_type => 'COSINE'\n","                )\n","            )\n","            SELECT\n","                'NVIDIA Docs' AS source_type,\n","                k.content,\n","                s.distance AS similarity_score,\n","                k.source_url\n","            FROM search_results s\n","            JOIN {self.NVIDIA_KNOWLEDGE_TABLE} k\n","              ON s.doc_id = k.doc_id\n","            ORDER BY s.distance ASC\n","        \"\"\"\n","\n","        # Query for Stack Overflow questions\n","        so_query = f\"\"\"\n","            WITH search_results AS (\n","                SELECT\n","                    base.doc_id,\n","                    distance\n","                FROM VECTOR_SEARCH(\n","                    TABLE {self.SO_EMBEDDINGS_TABLE},\n","                    'embedding',\n","                    (SELECT [{embedding_str}] AS query_vector),\n","                    top_k => {top_k},\n","                    distance_type => 'COSINE'\n","                )\n","            )\n","            SELECT\n","                'Stack Overflow' AS source_type,\n","                CONCAT('**Question:** ', k.title, '\\\\n\\\\n**Answer:** ', k.answer) AS content,\n","                s.distance AS similarity_score,\n","                k.source_url\n","            FROM search_results s\n","            JOIN {self.SO_KNOWLEDGE_TABLE} k\n","              ON s.doc_id = k.doc_id\n","            ORDER BY s.distance ASC\n","        \"\"\"\n","\n","\n","        # Run queries in parallel\n","        nvidia_job = self.client.query(nvidia_query)\n","        so_job = self.client.query(so_query)\n","\n","        # Combine and sort results\n","        all_results = list(nvidia_job.result()) + list(so_job.result())\n","        all_results.sort(key=lambda x: x.similarity_score)\n","\n","        return all_results[:top_k]\n","\n","    def _display_search_metrics(self, docs):\n","        table = Table(title=\"📊 Search & Retrieval Metrics\", show_header=True, header_style=\"bold magenta\")\n","        table.add_column(\"#\", style=\"dim\", width=3)\n","        table.add_column(\"Source\", style=\"bold blue\", width=15)\n","        table.add_column(\"Document Snippet\", style=\"cyan\", no_wrap=True, width=70)\n","        table.add_column(\"Source URL\", style=\"green\", no_wrap=True, width=50)\n","        table.add_column(\"Confidence\", justify=\"right\", style=\"bold yellow\")\n","\n","        for i, doc in enumerate(docs, 1):\n","            confidence = (1 - doc.similarity_score) * 100\n","            snippet = doc.content.replace('\\n', ' ').strip()\n","\n","            confidence_text = f\"{confidence:.1f}%\"\n","            if confidence > 75:\n","                color = \"green\"\n","            elif confidence > 50:\n","                color = \"yellow\"\n","            else:\n","                color = \"red\"\n","\n","            table.add_row(\n","                str(i),\n","                doc.source_type,\n","                snippet[:68] + \"...\" if len(snippet) > 70 else snippet,\n","                doc.source_url,\n","                Text(confidence_text, style=color)\n","            )\n","        self.console.print(table)\n","\n","    def _display_performance_metrics(self, timings):\n","        table = Table(title=\"⏱️ Performance Metrics\", show_header=True, header_style=\"bold blue\")\n","        table.add_column(\"Stage\", style=\"cyan\")\n","        table.add_column(\"Duration (s)\", style=\"magenta\", justify=\"right\")\n","        table.add_column(\"Percentage\", style=\"green\", justify=\"right\")\n","\n","        total_time = sum(timings.values())\n","        for stage, duration in timings.items():\n","            percentage = (duration / total_time * 100) if total_time > 0 else 0\n","            table.add_row(stage, f\"{duration:.3f}\", f\"{percentage:.1f}%\")\n","\n","        table.add_section()\n","        table.add_row(\"Total\", f\"{total_time:.3f}\", \"100.0%\")\n","        self.console.print(table)\n","\n","    def generate_answer(self, question):\n","        \"\"\"Generate conversational answer using ONLY BigQuery embeddings context\"\"\"\n","\n","        timings = {}\n","\n","        # --- 1. Vector Search ---\n","        start_time = time.time()\n","        similar_docs = self.search_similar_documents(question, top_k=3)\n","        timings['Vector Search'] = time.time() - start_time\n","\n","        if not similar_docs:\n","            self.console.print(\"[bold red]I couldn't find relevant information in our NVIDIA documentation.[/bold red]\")\n","            return\n","\n","        self._display_search_metrics(similar_docs)\n","\n","        # --- 2. Build context from search results ---\n","        start_time = time.time()\n","        context_parts = [f\"source: {doc.source_url}\\ncontent: {doc.content}\" for doc in similar_docs]\n","        sources = [f\"source: {doc.source_url}\\n\" for doc in similar_docs]\n","        timings['Content Fetching'] = time.time() - start_time\n","        context_text = \"\\n\\n---\\n\\n\".join(context_parts)\n","        sources_text = \"\\n\\n---\\n\\n\".join(sources)\n","        self.console.print(Panel(sources_text, title=\"[bold blue]📝 Context for LLM[/bold blue]\", expand=False))\n","\n","        prompt = f\"\"\"\n","        **Role**: You are an enthusiastic NVIDIA GPU expert assistant. You love helping developers with CUDA, GPU programming, and AI technologies.\n","\n","        **Context from NVIDIA Documentation**:\n","        {context_text}\n","\n","        **User Question**: {question}\n","\n","        **Instructions**:\n","        - Answer conversationally and helpfully, like a knowledgeable colleague.\n","        - Use bullet points or numbered steps when explaining complex topics.\n","        - Show enthusiasm for NVIDIA technologies.\n","        - Keep it professional but friendly.\n","        - Use emojis sparingly to make it engaging.\n","        - Always base your answer strictly on the context provided.\n","\n","        **Your Response**:\n","        \"\"\"\n","\n","        # --- 3. Answer Generation ---\n","        start_time = time.time()\n","        response = self.gen_model.generate_content(\n","            prompt,\n","            generation_config={\n","                \"temperature\": 0.3,\n","                \"max_output_tokens\": 1024,\n","                \"top_p\": 0.9\n","            }\n","        )\n","        timings['Answer Generation'] = time.time() - start_time\n","\n","        self.console.print(Panel(Markdown(response.text), title=\"[bold green]💡 NVIDIA AI Assistant Says...[/bold green]\"))\n","        self._display_performance_metrics(timings)\n","\n","def run_assistant():\n","    expert = NVIDIAExpertSystem()\n","    expert.console.print(Panel(\"[[bold green]🚀 NVIDIA AI Assistant Initialized[/bold green]]\", title=\"✅ System Ready\", expand=False))\n","\n","    questions = [\n","        \"What is CUDA memory coalescing and why is it important?\",\n","        \"How can I optimize CUDA kernels for better performance?\",\n","        \"What are the differences between shared memory and global memory in CUDA?\",\n","        \"Explain the concept of warp divergence in CUDA.\",\n","        \"How do CUDA streams help with concurrency?\",\n","    ]\n","\n","    # --- UI Components ---\n","    question_buttons = [widgets.Button(description=q, layout=widgets.Layout(width='95%')) for q in questions]\n","    custom_question_text = widgets.Text(placeholder='Or type your own question here...', layout=widgets.Layout(width='70%'))\n","    custom_question_button = widgets.Button(description=\"Ask Assistant\", button_style='success')\n","    output_area = widgets.Output()\n","\n","    def ask_question(question_text):\n","        with output_area:\n","            clear_output()\n","            expert.console.print(Panel(f\"[bold yellow]❓ Asking[/bold yellow]: {question_text}\", title=\"User Question\"))\n","            expert.generate_answer(question_text)\n","\n","    def on_button_clicked(b):\n","        ask_question(b.description)\n","\n","    def on_custom_button_clicked(b):\n","        if custom_question_text.value:\n","            ask_question(custom_question_text.value)\n","\n","    for btn in question_buttons:\n","        btn.on_click(on_button_clicked)\n","    custom_question_button.on_click(on_custom_button_clicked)\n","\n","    # --- Layout ---\n","    expert.console.print(Panel(\"[bold cyan]Select a question or enter your own below:[/bold cyan]\"))\n","    buttons_box = widgets.VBox(question_buttons)\n","    custom_input_box = widgets.HBox([custom_question_text, custom_question_button])\n","    display(buttons_box, custom_input_box, output_area)\n","\n","if __name__ == \"__main__\":\n","    run_assistant()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["f06b74b2cb694f379ee3832800e53d34","efbe3955664a4e0f9796499e06cc41c6","347964ad3f844cb38126bf474be4a4f0","76170ad5d44948599f1cd9ccdcbf95be","7403dc4df4bf41d7b69f743d06f6dfc3","3214db75609a46ffb381c133e45a1e29","daf000d2a5f241b2b6fb31569d5a2791","e3d42cb97e8849eb9e9547328534a266","b4b84e33b2a94d188b22a401e117010c","21d9ca7b628643c29c240985ae1af879","0bf1813ad31b4b5d9ffb627eb1469d06","23ec73589ef74bbb9a0208b6ec6ea31a","3bced61e52e8457c8a393ee2d9e7b40a","7b60cf4b580e4e14afd20da36b767ed5","804269e39f8a45b7bbdf6e1649065ac1","545f47e4541148bf85e4c3e0f4a75c52","7118b07b7e974b688d0be610af7049f7","c6f64945e6f94a63bb9033f59f35ba7d","4df05d0b5c71458aade0df44f2064ecd","068dae937a3a41eea667ca46f76aa099","8427c50db6a141d88947b85ed155c22c","68ab082458a549358e25080469864c16","65344159193f4d6da56192d676937277","aa8e5e85b7da454cbbeba07f0c20ac5d","90d5bc10f06745859a9c6837d7344950","10783e6972514b4eab5e373d8fc10bc8","44d27420a8794269af92f6f9a23203f0"]},"id":"MAOXyDsE-8fg","executionInfo":{"status":"ok","timestamp":1758058311400,"user_tz":420,"elapsed":171,"user":{"displayName":"R A","userId":"16665474501315354161"}},"outputId":"6b664e96-5546-49b2-9944-140c981305de"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["🔑 Fetching temporary access token from: https://bq-token-vendor-987726911762.us-central1.run.app/token\n","✅ Successfully fetched temporary token!\n","📊 Initializing BigQuery client with temporary credentials...\n","✅ BigQuery client is ready.\n","🚀 Initializing Vertex AI...\n","✅ Vertex AI initialized!\n"]},{"output_type":"display_data","data":{"text/plain":["╭────────── ✅ System Ready ───────────╮\n","│ [\u001b[1;32m🚀 NVIDIA AI Assistant Initialized\u001b[0m] │\n","╰──────────────────────────────────────╯\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭────────── ✅ System Ready ───────────╮\n","│ [<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">🚀 NVIDIA AI Assistant Initialized</span>] │\n","╰──────────────────────────────────────╯\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n","│ \u001b[1;36mSelect a question or enter your own below:\u001b[0m                                                                      │\n","╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n","│ <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Select a question or enter your own below:</span>                                                                      │\n","╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Button(description='What is CUDA memory coalescing and why is it important?', layout=Layout(wid…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f06b74b2cb694f379ee3832800e53d34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["HBox(children=(Text(value='', layout=Layout(width='70%'), placeholder='Or type your own question here...'), Bu…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6f64945e6f94a63bb9033f59f35ba7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10783e6972514b4eab5e373d8fc10bc8"}},"metadata":{}}]}]}